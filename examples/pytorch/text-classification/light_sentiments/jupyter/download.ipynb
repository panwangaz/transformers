{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f971977c-8bf9-4842-9857-01243b11f2a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'field' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m softmax\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDataTrainingArguments\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    Arguments pertaining to what data we are going to input our model for training and eval.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    the command line.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     task_name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m field(\n\u001b[1;32m     36\u001b[0m         default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     37\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhelp\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe name of the task to train on: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(task_to_keys\u001b[38;5;241m.\u001b[39mkeys())},\n\u001b[1;32m     38\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m, in \u001b[0;36mDataTrainingArguments\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDataTrainingArguments\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    Arguments pertaining to what data we are going to input our model for training and eval.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    the command line.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     task_name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfield\u001b[49m(\n\u001b[1;32m     36\u001b[0m         default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     37\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhelp\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe name of the task to train on: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(task_to_keys\u001b[38;5;241m.\u001b[39mkeys())},\n\u001b[1;32m     38\u001b[0m     )\n\u001b[1;32m     39\u001b[0m     dataset_name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m field(\n\u001b[1;32m     40\u001b[0m         default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhelp\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe name of the dataset to use (via the datasets library).\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     41\u001b[0m     )\n\u001b[1;32m     42\u001b[0m     dataset_config_name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m field(\n\u001b[1;32m     43\u001b[0m         default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhelp\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe configuration name of the dataset to use (via the datasets library).\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     44\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'field' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    # Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers.data.data_collator import DataCollator, DataCollatorWithPadding, default_data_collator\n",
    "from trainer import Trainer\n",
    "from scipy.special import softmax\n",
    "\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "\n",
    "    Using `HfArgumentParser` we can turn this class\n",
    "    into argparse arguments to be able to specify them on\n",
    "    the command line.\n",
    "    \"\"\"\n",
    "\n",
    "    task_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The name of the task to train on: \" + \", \".join(task_to_keys.keys())},\n",
    "    )\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                \"than this will be truncated, sequences shorter will be padded.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Whether to pad all samples to `max_seq_length`. \"\n",
    "                \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_predict_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    train_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"A csv or a json file containing the training data.\"}\n",
    "    )\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"A csv or a json file containing the validation data.\"}\n",
    "    )\n",
    "    test_file: Optional[str] = field(default=None, metadata={\"help\": \"A csv or a json file containing the test data.\"})\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.task_name is not None:\n",
    "            self.task_name = self.task_name.lower()\n",
    "            if self.task_name not in task_to_keys.keys():\n",
    "                raise ValueError(\"Unknown task, you should pick one in \" + \",\".join(task_to_keys.keys()))\n",
    "        elif self.dataset_name is not None:\n",
    "            pass\n",
    "        elif self.train_file is None or self.validation_file is None:\n",
    "            raise ValueError(\"Need either a GLUE task, a training/validation file or a dataset name.\")\n",
    "        else:\n",
    "            train_extension = self.train_file.split(\".\")[-1]\n",
    "            assert train_extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
    "            validation_extension = self.validation_file.split(\".\")[-1]\n",
    "            assert (\n",
    "                validation_extension == train_extension\n",
    "            ), \"`validation_file` should have the same extension (csv or json) as `train_file`.\"\n",
    "\n",
    "def format_feature(feature):\n",
    "    if isinstance(feature, float):\n",
    "        feature = round(feature, 3)\n",
    "        return feature\n",
    "\n",
    "def load_prelabel_output_dir(path):\n",
    "    files = [os.path.join(path, i) for i in os.listdir(path) if '.json' in i]\n",
    "    with open(files[0], \"r\") as jf:\n",
    "        data = json.load(jf)\n",
    "    for file in files[1:]:\n",
    "        with open(file, \"r\") as jf:\n",
    "            data_new = json.load(jf)\n",
    "            data.update(data_new)\n",
    "    return data\n",
    "\n",
    "def clean_string(sentence):\n",
    "    cleaned_string = re.sub(r'\\d+\\.\\s*', '', sentence)\n",
    "    cleaned_string = cleaned_string.replace(\"\\n\", \"\")\n",
    "    return cleaned_string\n",
    "\n",
    "def truncate_dialogue(string, max_len):\n",
    "    # 截取最后512个单词\n",
    "    words = string.split()\n",
    "    if len(words) < max_len:\n",
    "        return string\n",
    "    words_to_keep = words[-max_len:]\n",
    "\n",
    "    # 将截取的单词重新组合成字符串\n",
    "    truncated_string = \" \".join(words_to_keep)\n",
    "\n",
    "    # 确保对话以[user]或[advisor]开头\n",
    "    index_user = truncated_string.find(\"[user]\")\n",
    "    index_advisor = truncated_string.find(\"[advisor]\")\n",
    "\n",
    "    # 如果找到了[user]或[advisor]开头的索引\n",
    "    if (index_user != -1) or (index_advisor != -1):\n",
    "        truncated_string = truncated_string[min(index_user, index_advisor):]\n",
    "\n",
    "    # 确保截取后的对话以[user]或[advisor]开头\n",
    "    if truncated_string.startswith(\"[user]\") or truncated_string.startswith(\"[advisor]\"):\n",
    "        return truncated_string\n",
    "\n",
    "    # 如果不是以[user]或[advisor]开头，则继续向后截取\n",
    "    for i in range(len(truncated_string)):\n",
    "        if truncated_string[i:].startswith(\"[user]\") or truncated_string[i:].startswith(\"[advisor]\"):\n",
    "            return truncated_string[i:]\n",
    "\n",
    "    # 如果未找到[user]或[advisor]开头，则返回空字符串\n",
    "    return \"\"\n",
    "\n",
    "def get_data(path,max_len=512):\n",
    "    data = load_prelabel_output_dir(path)\n",
    "    sentences = []\n",
    "    scores = []\n",
    "    order_ids = []\n",
    "    sample_nums = []\n",
    "    for order_id in data:\n",
    "        for sample_num in data[order_id]:\n",
    "            history = clean_string(data[order_id][sample_num]['history'])\n",
    "            current = clean_string(data[order_id][sample_num]['current'])\n",
    "            score = data[order_id][sample_num]['score']\n",
    "            \n",
    "            if score > 0.3:\n",
    "                score = 2\n",
    "            elif score < -0.3:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = 1\n",
    "            \n",
    "            # sentence = history + current\n",
    "            # sentence = \"{}{}[SEP] {}\".format(history,current,current)\n",
    "            sentence = \"{}[SEP] {}\".format(history,current)\n",
    "            sentence = truncate_dialogue(sentence,max_len)\n",
    "            sentences.append(sentence)\n",
    "            scores.append(score)\n",
    "            order_ids.append(order_id)\n",
    "            sample_nums.append(sample_num)\n",
    "            \n",
    "    return sentences,scores,order_ids,sample_nums\n",
    "\n",
    "def collate_data_deprecated(path, max_len=512):\n",
    "\n",
    "    data = {\n",
    "        \"sentence\": [],\n",
    "        \"label\": [],\n",
    "        \"idx\": [],\n",
    "        \"group_id\": [] # 0 for raw data 1 for copied data\n",
    "    }\n",
    "\n",
    "    # balance means balancing the possitive and negative data\n",
    "    \n",
    "    raw_data,labels,order_ids,sample_nums = get_data(path,max_len)\n",
    "\n",
    "    data_idx = [i for i in range(len(raw_data))]\n",
    "\n",
    "    label_to_data_idxs = {}\n",
    "\n",
    "    for idx in data_idx:\n",
    "        label = labels[idx]\n",
    "        data[\"sentence\"].append(raw_data[idx])\n",
    "        data[\"label\"].append(label)\n",
    "        data[\"idx\"].append(idx)\n",
    "        data[\"group_id\"].append(0)\n",
    "        if label not in label_to_data_idxs:\n",
    "            label_to_data_idxs[label] = []\n",
    "        label_to_data_idxs[label].append(idx)\n",
    "\n",
    "    for label, idx_list in label_to_data_idxs.items():\n",
    "        print(\"ratio of {} label is: {}\".format(label, len(idx_list) / len(data_idx)))\n",
    "    \n",
    "    return data,order_ids,sample_nums\n",
    "\n",
    "def get_my_dataset(train_data_path,valid_data_path,max_len):\n",
    "    dataset_dict = {}\n",
    "    \n",
    "    train_data,train_order_id,train_sample_num = collate_data_deprecated(\n",
    "        path=train_data_path,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    \n",
    "    valid_data,valid_order_id,valid_sample_num = collate_data_deprecated(\n",
    "        path=valid_data_path,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "    dataset_dict[\"train_pre\"] = Dataset.from_pandas(pd.DataFrame(train_data))\n",
    "    dataset_dict[\"valid_pre\"] = Dataset.from_pandas(pd.DataFrame(valid_data))\n",
    "\n",
    "    return datasets.DatasetDict(dataset_dict),train_order_id,train_sample_num,valid_order_id,valid_sample_num\n",
    "\n",
    "def my_metrics(pred_labels, true_labels): \n",
    "    metric_dict = {}\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, pred_labels, average=None)\n",
    "    metric_dict['precision_negative'] = precision[0]\n",
    "    metric_dict['recall_negative'] = recall[0]\n",
    "    metric_dict['f1_score_negative'] = f1_score[0]\n",
    "    metric_dict['precision_neutral'] = precision[1]\n",
    "    metric_dict['recall_neutral'] = recall[1]\n",
    "    metric_dict['f1_score_neutral'] = f1_score[1]\n",
    "    metric_dict['precision_positive'] = precision[2]\n",
    "    metric_dict['recall_positive'] = recall[2]\n",
    "    metric_dict['f1_score_positive'] = f1_score[2]\n",
    "    \n",
    "    return metric_dict\n",
    "        \n",
    "\n",
    "# You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n",
    "# predictions and label_ids field) and has to return a dictionary string to float.\n",
    "def compute_metrics(p: EvalPrediction, group_id_list: list):\n",
    "    id2label_tb = {0: \"negative\", 1: \"neutral\", 2:\"positive\"}\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n",
    "    if data_args.task_name is not None:\n",
    "        result = metric.compute(predictions=preds, references=p.label_ids)\n",
    "        # compute my metrics here\n",
    "        my_result = my_metrics(preds, p.label_ids)\n",
    "        print(my_result)\n",
    "        for a_key in list(my_result.keys()):\n",
    "            result[a_key] = my_result[a_key]\n",
    "        # if len(result) > 1:\n",
    "        #     result[\"combined_score\"] = np.mean(list(result.values())).item()\n",
    "        return result\n",
    "    elif is_regression:\n",
    "        return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n",
    "    else:\n",
    "        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n",
    "\n",
    "def main():\n",
    "    model_path = './tmp/detect_flirt.finbert.v2.model/'\n",
    "    train_data_path = '/data/yangxiaoran/nonrt_init/pay_rate_level_train/data/sentiment_train_new/'\n",
    "    valid_data_path = '/data/yangxiaoran/nonrt_init/pay_rate_level_train/data/sentiment_valid_new/'\n",
    "    max_len = 512\n",
    "    parser = HfArgumentParser(TrainingArguments, CustomerArguments)\n",
    "    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
    "        # If we pass only one argument to the script and it's the path to a json file,\n",
    "        # let's parse it to get our arguments.\n",
    "        training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
    "    else:\n",
    "        training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "    raw_datasets,train_order_id,train_sample_num,valid_order_id,valid_sample_num = get_my_dataset(train_data_path,valid_data_path,max_len)\n",
    "    \n",
    "    # 加载配置文件\n",
    "    config = AutoConfig.from_pretrained(model_path)\n",
    "    \n",
    "    # 加载分词器\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # 加载模型\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "    # Initialize our Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=None,\n",
    "        eval_dataset=None,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        kick_ratio=0\n",
    "    )\n",
    "    \n",
    "    train_predict_dataset = raw_datasets[\"train_pre\"]\n",
    "    valid_predict_dataset = raw_datasets[\"valid_pre\"]\n",
    "    \n",
    "    order_id_2_feature={}\n",
    "    order_id_predict={}\n",
    "    feature_name=['bert_'+str(i) for i in range(768)]\n",
    "    \n",
    "    predict_datasets = [train_predict_dataset,valid_predict_dataset]\n",
    "    order_id_list = [train_order_id,valid_order_id]\n",
    "    sample_num_list = [train_sample_num,valid_sample_num]\n",
    "\n",
    "    for predict_dataset, order_id_data, sample_num_data in zip(predict_datasets, order_id_list, sample_num_list):\n",
    "        # Removing the `label` columns because it contains -1 and Trainer won't like that.\n",
    "        predict_dataset_new = predict_dataset.remove_columns(\"label\")\n",
    "        predictions,hidden_states = trainer.predict(predict_dataset_new, metric_key_prefix=\"predict\")\n",
    "        predictions = predictions.predictions\n",
    "        \n",
    "        # predictions = np.squeeze(predictions) if is_regression else np.argmax(predictions, axis=1)\n",
    "        predictions = np.squeeze(predictions) if is_regression else softmax(predictions, axis=1)\n",
    "\n",
    "        if trainer.is_world_process_zero():\n",
    "            for index, item in enumerate(tqdm(predictions)):\n",
    "                features = {}\n",
    "                \n",
    "                if order_id_data[index] not in order_id_predict:\n",
    "                    order_id_predict[order_id_data[index]] = {}\n",
    "                if sample_num_data[index] not in order_id_predict[order_id_data[index]]:\n",
    "                    order_id_predict[order_id_data[index]][sample_num_data[index]] = {}\n",
    "                order_id_predict[order_id_data[index]][sample_num_data[index]]['true'] = int(predict_dataset['label'][index])\n",
    "                \n",
    "                # order_id_predict[order_id_data[index]][sample_num_data[index]]['predict'] = int(item)\n",
    "                order_id_predict[order_id_data[index]][sample_num_data[index]]['predict'] = list(item.astype(np.float))\n",
    "                \n",
    "                for i in range(768):\n",
    "                    features[feature_name[i]] = list(hidden_states[index].astype(float))[i]\n",
    "                if order_id_data[index] not in order_id_2_feature:\n",
    "                    order_id_2_feature[order_id_data[index]] = {}\n",
    "                if sample_num_data[index] not in order_id_2_feature[order_id_data[index]]:\n",
    "                    order_id_2_feature[order_id_data[index]][sample_num_data[index]] = features\n",
    "\n",
    "    with open('./data/sentence_feature_try.json', 'w') as f:\n",
    "        json.dump(order_id_2_feature, f)\n",
    "    with open('./tmp/sst2/order_id_predict_try.json', 'w') as f:\n",
    "        json.dump(order_id_predict, f)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
